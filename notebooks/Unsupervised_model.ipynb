{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4b89b6",
   "metadata": {},
   "source": [
    "# Packages imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e666316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import string \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99eaeb",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f32413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deals_raw = pd.read_excel('../raw_data/deals_raw.xlsx')\n",
    "key_match = pd.read_excel('../raw_data/new_keywords.xlsx')\n",
    "invest_key = pd.read_excel('../raw_data/invest_profile_keywords.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4676c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_table = pd.read_excel('../raw_data/matching_table.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9e081",
   "metadata": {},
   "source": [
    "# Preprocessing of DataFrames and training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af371fb5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23d0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e13a090",
   "metadata": {},
   "source": [
    "## Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a543ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6863/944302302.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_deals_raw_work.dropna(subset='target_name',inplace=True)\n",
      "/tmp/ipykernel_6863/944302302.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_deals_raw_work['strs'] = df_deals_raw_work['strs'].str.replace(',',' ')\n",
      "/tmp/ipykernel_6863/944302302.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_deals_raw_work[['target_ebit','target_revenue']] = df_deals_raw_work[['target_ebit','target_revenue']].replace(np.nan, 0)\n",
      "/tmp/ipykernel_6863/944302302.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_deals_raw_work['strs'] = df_deals_raw_work['strs'].apply(remove_punctuations)\n",
      "/tmp/ipykernel_6863/944302302.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_deals_raw_work['strs'] = df_deals_raw_work['strs'].apply(lambda x: x.lower())\n"
     ]
    }
   ],
   "source": [
    "rel_columns = ['target_name', 'name', 'target_ebit', 'target_ebitda', 'target_revenue']\n",
    "\n",
    "dict_2 = {}\n",
    "dict_3 = {}\n",
    "dict_4 = {}\n",
    "\n",
    "for i in df_deals_raw['target_name'].unique():\n",
    "    df_rel = df_deals_raw.loc[df_deals_raw['target_name'] == i]\n",
    "    set_rel_2 = set(df_rel['name_de.2'])\n",
    "    dict_2[i] = set_rel_2\n",
    "\n",
    "    set_rel_3 = set(df_rel['name_de.3'])\n",
    "    dict_3[i] = set_rel_3\n",
    "\n",
    "    set_rel_4 = set(df_rel['name_de.4'])\n",
    "    dict_4[i] = set_rel_4\n",
    "    \n",
    "df_deals_raw_rel = df_deals_raw[rel_columns].copy().drop_duplicates(subset = 'name')\n",
    "df_deals_raw_rel['2'] = df_deals_raw_rel.target_name.apply(lambda x: dict_2.get(x))\n",
    "df_deals_raw_rel['3'] = df_deals_raw_rel.target_name.apply(lambda x: dict_3.get(x))\n",
    "df_deals_raw_rel['4'] = df_deals_raw_rel.target_name.apply(lambda x: dict_4.get(x))\n",
    "df_deals_raw_rel['comb'] = df_deals_raw_rel.apply(lambda x: set.union(x['2'], x['3'], x['4']), axis = 1)\n",
    "df_deals_raw_rel['comb_str'] = df_deals_raw_rel.comb.apply(lambda x: list(map(str, x)))\n",
    "df_deals_raw_rel['strs'] = df_deals_raw_rel.comb_str.apply(lambda x:  \",\".join(list(x)))\n",
    "df_deals_raw_rel.drop_duplicates(subset='target_name', keep=\"first\",inplace=True)\n",
    "\n",
    "df_deals_raw_work = df_deals_raw_rel[['target_name','target_ebit','target_revenue','strs']]\n",
    "\n",
    "df_deals_raw_work.dropna(subset='target_name',inplace=True)\n",
    "df_deals_raw_work['strs'] = df_deals_raw_work['strs'].str.replace(',',' ')\n",
    "df_deals_raw_work[['target_ebit','target_revenue']] = df_deals_raw_work[['target_ebit','target_revenue']].replace(np.nan, 0)\n",
    "df_deals_raw_work.reset_index().drop('index',axis=1)\n",
    "\n",
    "df_deals_raw_work['strs'] = df_deals_raw_work['strs'].apply(remove_punctuations) \n",
    "df_deals_raw_work['strs'] = df_deals_raw_work['strs'].apply(lambda x: x.lower())\n",
    "\n",
    "stop_words = set(stopwords.words('german')) \n",
    "\n",
    "for name_de in df_deals_raw_work['strs']:    \n",
    "    word_tokens = word_tokenize(name_de) \n",
    "    name_de = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "scaler = MinMaxScaler()\n",
    "df_deals_raw_work_fin = pd.DataFrame(scaler.fit_transform(\n",
    "                        df_deals_raw_work.drop(columns=['target_name','strs'])),\n",
    "                                    columns=['target_ebit', 'target_revenue'])\n",
    "\n",
    "df_deals_raw_work_fin = df_deals_raw_work_fin.reset_index().drop('index',axis=1)\n",
    "\n",
    "df_deals_raw_work = df_deals_raw_work.reset_index().drop('index',axis=1)\n",
    "df_deals_raw_work1 = pd.concat([df_deals_raw_work.drop(columns=['target_ebit','target_revenue']), \n",
    "                                df_deals_raw_work_fin], axis=1)\n",
    "\n",
    "vec_tf_target = TfidfVectorizer()\n",
    "x_tf_target = vec_tf_target.fit_transform(df_deals_raw_work1['strs'])\n",
    "df_wrds_tf_target = pd.DataFrame(x_tf_target.toarray(), columns=vec_tf_target.get_feature_names_out())\n",
    "df_tf_target = pd.concat([df_deals_raw_work1, df_wrds_tf_target], axis=1)\n",
    "df_tf_target.drop('strs', axis=1, inplace=True)\n",
    "df_tf_target_4pca = df_tf_target.drop('target_name',axis=1)\n",
    "\n",
    "pca_tf_target = PCA()\n",
    "df_tf_target_pca = pca_tf_target.fit_transform(df_tf_target_4pca)\n",
    "total_explained_variance = pca_tf_target.explained_variance_ratio_.cumsum()\n",
    "n_over_95 = len(total_explained_variance[total_explained_variance>=.95])\n",
    "n_to_reach_95 = df_tf_target_pca.shape[1] - n_over_95\n",
    "pca_tf_target = PCA(n_components=n_to_reach_95)\n",
    "df_tf_target_pca = pca_tf_target.fit_transform(df_tf_target_4pca)\n",
    "\n",
    "nn_target_tf_pca = NearestNeighbors(n_neighbors=10).fit(df_tf_target_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648f373",
   "metadata": {},
   "source": [
    "## Investors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d9b9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_key['name_de'] = invest_key['name_de'].replace(dict(zip(key_match.name_de,key_match.new_keyword)))\n",
    "invest_key_small = invest_key[['name','name_de']]\n",
    "invest_key_concat = invest_key_small.astype(str).groupby('name').agg({'name_de':', '.join})\n",
    "    \n",
    "invest_key_concat1 = invest_key_concat['name_de'].str.replace('nan','').reset_index()\n",
    "invest_key_concat1['name_de'] = invest_key_concat1['name_de'].apply(remove_punctuations)        \n",
    "invest_key_concat1['name_de'].replace(r'^\\s*$',np.nan,regex=True,inplace=True)\n",
    "invest_key_concat1 = invest_key_concat1.dropna().reset_index().drop('index',axis=1)\n",
    "invest_key_concat1['name_de'] = invest_key_concat1['name_de'].apply(lambda x: x.lower())\n",
    "\n",
    "stop_words = set(stopwords.words('german')) \n",
    "\n",
    "for name_de in invest_key_concat1['name_de']:    \n",
    "    word_tokens = word_tokenize(name_de) \n",
    "    name_de = [w for w in word_tokens if not w in stop_words] \n",
    "    \n",
    "vec_tf_key = TfidfVectorizer()\n",
    "x_tf_key = vec_tf_key.fit_transform(invest_key_concat1['name_de'])\n",
    "df_wrds_tf_key = pd.DataFrame(x_tf_key.toarray(), columns=vec_tf_key.get_feature_names_out())\n",
    "df_tf_key = pd.concat([invest_key_concat1, df_wrds_tf_key], axis=1)\n",
    "df_tf_key.drop('name_de', axis=1, inplace=True)\n",
    "df_tf_key_4pca = df_tf_key.drop('name',axis=1)\n",
    "\n",
    "pca_tf_key = PCA()\n",
    "df_tf_key_pca = pca_tf_key.fit_transform(df_tf_key_4pca)\n",
    "total_explained_variance = pca_tf_key.explained_variance_ratio_.cumsum()\n",
    "n_over_95 = len(total_explained_variance[total_explained_variance>=.95])\n",
    "n_to_reach_95 = df_tf_key_pca.shape[1] - n_over_95\n",
    "pca_tf_key = PCA(n_components=n_to_reach_95)\n",
    "df_tf_key_pca = pca_tf_key.fit_transform(df_tf_key_4pca)\n",
    "\n",
    "qty_of_nn = 5\n",
    "nn_key_tf_pca = NearestNeighbors(n_neighbors=qty_of_nn).fit(df_tf_key_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0ea2e",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "759dcd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6863/3289453189.py:57: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  if invest_key_concat1['name'].str.contains(investor).any():\n",
      "/tmp/ipykernel_6863/3289453189.py:66: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  if invest_key_concat1['name'].str.contains(investor).any():\n"
     ]
    }
   ],
   "source": [
    "target_pred = df_deals_raw_work1[df_deals_raw_work1['target_name']=='Maybach-Luxury']\n",
    "\n",
    "target_pred_scapled = pd.DataFrame(scaler.transform(target_pred.drop(columns=['target_name','strs'])),\n",
    "                                  columns=['target_ebit', 'target_revenue'])\n",
    "\n",
    "tx_to_pred = vec_tf_target.transform(target_pred['strs'])\n",
    "df_target_pred_wrds = pd.DataFrame(tx_to_pred.toarray(),columns=vec_tf_target.get_feature_names_out())\n",
    "df_target_pred = df_target_pred_wrds.copy()\n",
    "df_target_pred.insert(0,'target_ebit',target_pred_scapled['target_ebit'])\n",
    "df_target_pred.insert(1,'target_revenue',target_pred_scapled['target_revenue'])\n",
    "df_target_pred_pca = pca_tf_target.transform(df_target_pred)\n",
    "\n",
    "nn_targets = nn_target_tf_pca.kneighbors(df_target_pred_pca)\n",
    "\n",
    "name = []\n",
    "description = []\n",
    "distance = []\n",
    "\n",
    "\n",
    "\n",
    "for x,y in zip(nn_targets[1][0],nn_targets[0][0]):\n",
    "    name.append(df_deals_raw_work1['target_name'].iloc[x])\n",
    "    description.append(df_deals_raw_work1['strs'].iloc[x])\n",
    "    distance.append(y)\n",
    "    \n",
    "    \n",
    "df_companies = pd.DataFrame({'name':name,\n",
    "               'description':description,\n",
    "               'distance':distance})\n",
    "\n",
    "\n",
    "matching_investors = []\n",
    "matching_target = []\n",
    "matching_distance = []\n",
    "for company in df_companies['name']:\n",
    "    next_investor = matching_table[(matching_table['target_name']==company) & (matching_table['deal_stage_id']==4)]['comp_name'].tolist()\n",
    "    matching_investors+=next_investor\n",
    "    matching_target+=len(next_investor)*[company]\n",
    "    next_distance = df_companies[df_companies['name']==company]['distance'].tolist()\n",
    "    matching_distance+=len(next_investor)*next_distance\n",
    "df_match_investors = pd.DataFrame({'investors':matching_investors,'targets':matching_target,'distance':matching_distance})\n",
    "\n",
    "\n",
    "if len(df_match_investors['investors'].unique())>=10:\n",
    "    best_investors = df_match_investors['investors'].unique()[:10].tolist()\n",
    "else:\n",
    "    best_investors = df_match_investors['investors'].unique().tolist()\n",
    "    \n",
    "name_investor = []\n",
    "description_investor = []\n",
    "distance_investor_investor = []\n",
    "distance_target_target = []\n",
    "\n",
    "\n",
    "for investor in best_investors:\n",
    "    name_investor.append(investor)\n",
    "    if invest_key_concat1['name'].str.contains(investor).any():\n",
    "        description_investor.append(invest_key_concat1[invest_key_concat1['name']==investor]['name_de'].to_list()[0])\n",
    "    else:\n",
    "        description_investor.append('Investor not in the list')\n",
    "    distance_investor_investor.append(0)\n",
    "    distance_target_target.append(df_match_investors[df_match_investors['investors']==investor]['distance'].min())\n",
    "\n",
    "\n",
    "for investor in best_investors:\n",
    "    if invest_key_concat1['name'].str.contains(investor).any():\n",
    "        first_distance = df_match_investors[df_match_investors['investors']==investor]['distance'].min()\n",
    "        to_pred = invest_key_concat1[invest_key_concat1['name']==investor]\n",
    "        x_to_pred = vec_tf_key.transform(to_pred['name_de'])\n",
    "        df_pred_wrds = pd.DataFrame(x_to_pred.toarray(),columns=vec_tf_key.get_feature_names_out())\n",
    "        df_pred = pd.concat([to_pred,df_pred_wrds], axis=1)\n",
    "        df_pred.drop('name_de', axis=1, inplace=True)\n",
    "        df_pred_4pca = df_pred.drop('name',axis=1).dropna()\n",
    "        df_pred_pca = pca_tf_key.transform(df_pred_4pca)\n",
    "\n",
    "        nn_investors = nn_key_tf_pca.kneighbors(df_pred_pca)\n",
    "\n",
    "\n",
    "        for x,y in zip(nn_investors[1][0],nn_investors[0][0]):\n",
    "            name_investor.append(invest_key_concat1['name'].iloc[x])\n",
    "            description_investor.append(invest_key_concat1['name_de'].iloc[x])\n",
    "            distance_investor_investor.append(y)\n",
    "            distance_target_target.append(first_distance)\n",
    "        \n",
    "df_investors = pd.DataFrame({'name':name_investor,\n",
    "               'description':description_investor,\n",
    "               'distance_investor<=>investor':distance_investor_investor,\n",
    "                'distance_target<=>target':distance_target_target,\n",
    "                'distance_target<=>investor': [a+b for a,b in zip(distance_investor_investor,distance_target_target)]})\n",
    "\n",
    "df_investors_sorted = df_investors.sort_values('distance_target<=>investor')\n",
    "df_investors_sorted.reset_index(inplace=True)\n",
    "df_investors_sorted.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a7aef55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>distance_investor&lt;=&gt;investor</th>\n",
       "      <th>distance_target&lt;=&gt;target</th>\n",
       "      <th>distance_target&lt;=&gt;investor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eso Capital UK Ltd</td>\n",
       "      <td>zubehör glas handel luxusgüter lizenz sme fizierung</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.002804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eso Capital UK Ltd</td>\n",
       "      <td>zubehör glas handel luxusgüter lizenz sme fizierung</td>\n",
       "      <td>0.020221</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.023025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aprima</td>\n",
       "      <td>personaldienstleister sme</td>\n",
       "      <td>0.968624</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>0.971428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Essence Capital</td>\n",
       "      <td>bewertung fizierung  strategic advisory fond</td>\n",
       "      <td>0.997415</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>1.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIT Leveraged Finance</td>\n",
       "      <td>fizierung fizierung  fizierung übernahme</td>\n",
       "      <td>1.021513</td>\n",
       "      <td>0.002804</td>\n",
       "      <td>1.024317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                                          description  \\\n",
       "0     Eso Capital UK Ltd  zubehör glas handel luxusgüter lizenz sme fizierung   \n",
       "1     Eso Capital UK Ltd  zubehör glas handel luxusgüter lizenz sme fizierung   \n",
       "2                 Aprima                            personaldienstleister sme   \n",
       "3        Essence Capital         bewertung fizierung  strategic advisory fond   \n",
       "4  CIT Leveraged Finance             fizierung fizierung  fizierung übernahme   \n",
       "\n",
       "   distance_investor<=>investor  distance_target<=>target  \\\n",
       "0                      0.000000                  0.002804   \n",
       "1                      0.020221                  0.002804   \n",
       "2                      0.968624                  0.002804   \n",
       "3                      0.997415                  0.002804   \n",
       "4                      1.021513                  0.002804   \n",
       "\n",
       "   distance_target<=>investor  \n",
       "0                    0.002804  \n",
       "1                    0.023025  \n",
       "2                    0.971428  \n",
       "3                    1.000219  \n",
       "4                    1.024317  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_investors_sorted.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "137px",
    "width": "378px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
